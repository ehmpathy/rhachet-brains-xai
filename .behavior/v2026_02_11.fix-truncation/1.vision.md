# vision: prove or disprove brain output truncation

## the outcome world

### before

- users report sporadic output truncation from brain.atom.ask
- no systematic evidence — anecdotal reports only
- unclear if defect is in:
  - this package (rhachet-brains-xai)
  - the upstream xai api
  - the caller's prompt/schema design
  - or random api variance
- developers hesitate to use this brain for production tasks

### after

- integration tests exercise long-output scenarios systematically
- tests measure output token counts vs expected ranges
- truncation behavior is documented:
  - if present: root cause identified, workaround or fix applied
  - if absent: confidence established via evidence
- developers trust the brain for production workloads

### the "aha" moment

the test suite runs and produces clear evidence:

```
genBrainAtom.truncation.integration
  ✓ output ≥ 100 tokens when expected (run 1: 312 tokens)
  ✓ output ≥ 100 tokens when expected (run 2: 289 tokens)
  ✓ output ≥ 100 tokens when expected (run 3: 301 tokens)
```

or, if truncation is real:

```
genBrainAtom.truncation.integration
  ✗ output ≥ 100 tokens when expected (run 1: 8 tokens) — TRUNCATED
  ✓ output ≥ 100 tokens when expected (run 2: 312 tokens)
  ✓ output ≥ 100 tokens when expected (run 3: 289 tokens)
```

either way — evidence replaces speculation.

---

## user experience

### usecases

| who | goal | contract |
|-----|------|----------|
| package maintainer | verify brain does not truncate | run `npm run test:integration -- truncation` |
| consumer | trust brain for brief compression | see green tests + documented behavior |
| investigator | diagnose sporadic truncation | reproduce with test, bisect to root cause |

### contract: the test

```ts
given('[case4] long output is requested', () => {
  when('[t0] prompt requests 200+ token response', () => {
    const result = useThen('it returns response', async () =>
      brainAtom.ask({
        role: {},
        prompt: `
write a detailed 200-word explanation of why seaweed is green.
include at least 3 specific points about chlorophyll.
output as a single paragraph.
        `,
        schema: { output: z.object({ content: z.string() }) },
      }),
    );

    then('output has ≥ 100 tokens', () => {
      // rough estimate: 1 token ≈ 4 chars
      const estimatedTokens = result.output.content.length / 4;
      expect(estimatedTokens).toBeGreaterThanOrEqual(100);
    });

    then('output is not truncated to first sentence', () => {
      // truncation symptom: output ends abruptly
      expect(result.output.content.split('.').length).toBeGreaterThan(3);
    });
  });
});
```

### timeline

1. **run test once** — see if truncation occurs
2. **run test 5x** — check for sporadic truncation (the reported symptom)
3. **if truncation found** — bisect to root cause
4. **if no truncation** — document confidence, close investigation

---

## mental model

### how users describe this

> "we ran the truncation tests — they passed 5/5 times. the brain does not truncate; the issue was in our prompt design."

or:

> "the truncation test caught it — 1 in 5 runs returns only 8 tokens. the xai api has a sporadic stop-token bug."

### analogies

- **smoke test for output length** — like a load test, but for token count
- **canary in the coal mine** — catches truncation before production

### terms

| we say | users might say |
|--------|-----------------|
| truncation | cut off, chopped, incomplete |
| token count | word count, length |
| sporadic | sometimes, randomly, flaky |

---

## evaluation

### how well does it solve the goal?

| goal | solution quality |
|------|-----------------|
| prove truncation exists | ✓ test will fail with evidence |
| disprove truncation | ✓ test will pass consistently |
| identify root cause | △ test narrows scope; bisection needed for root cause |
| fix truncation | ○ separate effort once cause is known |

### pros

- cheap to implement (one integration test)
- deterministic evidence replaces speculation
- reusable as regression guard

### cons

- requires multiple runs to catch sporadic issues
- test can pass locally but fail in ci (env variance)
- does not automatically fix the issue — just proves it

### edge cases

| edge case | pit of success |
|-----------|---------------|
| api rate limits | test uses small payload, fast model |
| network flakiness | test retries once on network error |
| prompt variance | test uses explicit length instruction |
| token count accuracy | test uses conservative threshold (100 vs 200) |

---

## next

once vision is approved, define blackbox criteria (acceptance tests) and blueprint (implementation plan).
